{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fb4083",
   "metadata": {},
   "source": [
    "# Part 1: DBSCAN â€” Pre-Processing\n",
    "\n",
    "## Contents\n",
    "\n",
    "[Merging Synonyms with WordNet](#merging-synonyms-to-reveal-hidden-patterns)\\\n",
    "[Improving the Set of Features in Other Ways](#improving-the-set-of-features-in-other-ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bbfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description\n",
       "0   1  Active classic boxers - There's a reason why o...\n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...\n",
       "2   3  Active sport briefs - These superbreathable no...\n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...\n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../../Data/sample-data.csv\")\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f395b",
   "metadata": {},
   "source": [
    "## Merging Synonyms to Reveal Hidden Patterns?\n",
    "\n",
    "We may be able to reduce the size of the vocabulary and reveal patterns that way.\n",
    "\n",
    "In order to be able to pick the right synonym to use in context, I need to see how well spaCy's sentence vector similarity method works. Using the two meanings of \"bank\" (a place where you store your money, and the banks of a river)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67272a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902679443359375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_1 = nlp(\"This cardigan is the best cardigan. It has impeccable seams and I would gladly wear it to the bank.\")\n",
    "sent_2 = nlp(\"The Bank of Ireland is a financial institution involved with many countries' international currency bonds.\")\n",
    "\n",
    "sent_1.similarity(sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916204214096069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_3 = nlp(\"This cardigan is the best cardigan. It has impeccable seams and I would gladly wear it to the banks of the Danube.\")\n",
    "sent_3.similarity(sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39141d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406769037246704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_4 = nlp(\"The river Boyne has seen many civilisations on its banks.\")\n",
    "sent_4.similarity(sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5076c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8120417594909668"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_3.similarity(sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36783286929130554"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_3.similarity(nlp(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b2cab",
   "metadata": {},
   "source": [
    "#### Without trying to merge synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91044d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df.copy()\n",
    "df[\"clean_docs\"] = df[\"description\"].str.replace(r\"[^a-zA-Z0-9']+\", \" \", regex=True)\\\n",
    "                                    .apply(lambda desc: nlp(desc.lower()))\\\n",
    "                                    .apply(lambda doc: [token.lemma_ for token in doc\n",
    "                                                        if token.text not in STOP_WORDS])\\\n",
    "                                    .apply(lambda ls: \" \".join(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24cf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>active classic boxer reason boxer cult favorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>active sport boxer brief skin glory require mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>active sport brief superbreathable fly brief m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>alpine guide pant skin climb ice switch rock t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>alpine wind jkt high ridge steep ice alpine ja...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  active classic boxer reason boxer cult favorit...  \n",
       "1  active sport boxer brief skin glory require mo...  \n",
       "2  active sport brief superbreathable fly brief m...  \n",
       "3  alpine guide pant skin climb ice switch rock t...  \n",
       "4  alpine wind jkt high ridge steep ice alpine ja...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45034d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_syn_merge_vectoriser = TfidfVectorizer(stop_words=\"english\")\n",
    "no_syn_merge_X = no_syn_merge_vectoriser.fit_transform(df[\"clean_docs\"])\n",
    "len(no_syn_merge_vectoriser.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e37631",
   "metadata": {},
   "source": [
    "It seems, though, that the lemmas in df have a few available synonyms on average, and that we should at least try to reduce the size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5883f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.695089691414051)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep this cell to motivate my choice of using wordnet synonyms\n",
    "test_descs = df[\"description\"].apply(lambda desc: nlp(desc))\\\n",
    "                              .apply(lambda doc: np.mean([len(wn.synsets(token.lemma_)) for token in doc])) \n",
    "test_descs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abeef8f",
   "metadata": {},
   "source": [
    "#### With trying to merge synonyms\n",
    "\n",
    "Using WordNet's Synsets feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f382fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/237vh0m92ws4lbbnrhz3b1rr0000gn/T/ipykernel_26862/3645039004.py:13: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarity_score = sent_context.similarity(definition_doc)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>[Synset('active.a.14'), Synset('classical.a.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>[Synset('active_voice.n.01'), Synset('sport.n....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>[Synset('active.a.01'), Synset('sport.n.04'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>[Synset('alpine.s.03'), Synset('guide.n.06'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>[Synset('alpine.s.03'), Synset('wind_instrumen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  [Synset('active.a.14'), Synset('classical.a.01...  \n",
       "1  [Synset('active_voice.n.01'), Synset('sport.n....  \n",
       "2  [Synset('active.a.01'), Synset('sport.n.04'), ...  \n",
       "3  [Synset('alpine.s.03'), Synset('guide.n.06'), ...  \n",
       "4  [Synset('alpine.s.03'), Synset('wind_instrumen...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_df.copy()\n",
    "\n",
    "def get_best_synonym(token):\n",
    "    sent_context = token.sent\n",
    "    synset_list = wn.synsets(token.lemma_)\n",
    "\n",
    "    if len(synset_list) <= 1:\n",
    "        return token.lemma_\n",
    "    \n",
    "    best, best_score = None, -1\n",
    "    for synonym in synset_list:\n",
    "        definition_doc = nlp(synonym.definition())\n",
    "        similarity_score = sent_context.similarity(definition_doc)\n",
    "        if similarity_score > best_score:\n",
    "            best, best_score = synonym, similarity_score\n",
    "    return best.name()\n",
    "\n",
    "df[\"clean_docs\"] = df[\"description\"].str.replace(r\"[^a-zA-Z0-9']+\", \" \", regex=True)\\\n",
    "                                    .apply(lambda desc: nlp(desc.lower()))\\\n",
    "                                    .apply(lambda doc: [token for token in doc\n",
    "                                                        if token.text not in STOP_WORDS])\\\n",
    "                                    .apply(lambda doc: [get_best_synonym(token) for token in doc])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>active.a.14 classical.a.01 packer.n.01 reason....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>active_voice.n.01 sport.n.04 packer.n.01 brief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>active.a.01 sport.n.04 brief.n.01 superbreatha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>alpine.s.03 guide.n.06 trouser.n.01 skin.n.01 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>alpine.s.03 wind_instrument.n.01 jkt high_gear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  active.a.14 classical.a.01 packer.n.01 reason....  \n",
       "1  active_voice.n.01 sport.n.04 packer.n.01 brief...  \n",
       "2  active.a.01 sport.n.04 brief.n.01 superbreatha...  \n",
       "3  alpine.s.03 guide.n.06 trouser.n.01 skin.n.01 ...  \n",
       "4  alpine.s.03 wind_instrument.n.01 jkt high_gear...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"clean_docs\"] = df[\"clean_docs\"].apply(lambda ls: \" \".join([syn.name() if (str(type(syn)) == \"<class 'nltk.corpus.reader.wordnet.Synset'>\")\n",
    "                                                               else syn\n",
    "                                                               for syn in ls]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>active classical packer reason packer cult fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>active_voice sport packer brief skin aura nece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>active sport brief superbreathable fly brief m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>alpine guide trouser skin climb methamphetamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>alpine wind_instrument jkt high_gear ridge ste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  active classical packer reason packer cult fro...  \n",
       "1  active_voice sport packer brief skin aura nece...  \n",
       "2  active sport brief superbreathable fly brief m...  \n",
       "3  alpine guide trouser skin climb methamphetamin...  \n",
       "4  alpine wind_instrument jkt high_gear ridge ste...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"clean_docs\"] = df[\"clean_docs\"].apply(lambda st: \" \".join([word.split(\".\")[0] if (\".\" in word)\n",
    "                                                               else word\n",
    "                                                               for word in st.split(\" \")]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57facd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3947"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_merge_vectoriser = TfidfVectorizer(stop_words=\"english\")\n",
    "syn_merge_X = syn_merge_vectoriser.fit_transform(df[\"clean_docs\"])\n",
    "len(syn_merge_vectoriser.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ca879",
   "metadata": {},
   "source": [
    "Using the context of the sentences actually somehow increases the dimensionality by quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb42977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>active_agent classic boxer reason boxer cult f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>active_agent sport boxer brief skin glory nece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>active_agent sport brief superbreathable fly b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>alpine usher pant skin ascent ice switch rock ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>alpine wind jkt high ridge steep ice alpine ja...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  active_agent classic boxer reason boxer cult f...  \n",
       "1  active_agent sport boxer brief skin glory nece...  \n",
       "2  active_agent sport brief superbreathable fly b...  \n",
       "3  alpine usher pant skin ascent ice switch rock ...  \n",
       "4  alpine wind jkt high ridge steep ice alpine ja...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_df.copy()\n",
    "\n",
    "def get_best_synonym(token):\n",
    "    sent_context = token.sent\n",
    "    synset_list = wn.synsets(token.lemma_)\n",
    "    return synset_list[0].name().split(\".\")[0] if len(synset_list) >= 1 else token.lemma_\n",
    "\n",
    "df[\"clean_docs\"] = df[\"description\"].str.replace(r\"[^a-zA-Z0-9']+\", \" \", regex=True)\\\n",
    "                                    .apply(lambda desc: nlp(desc.lower()))\\\n",
    "                                    .apply(lambda doc: [token for token in doc\n",
    "                                                        if token.text not in STOP_WORDS])\\\n",
    "                                    .apply(lambda doc: [get_best_synonym(token) for token in doc])\\\n",
    "                                    .apply(lambda ls: \" \".join(ls))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567e95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_merge_vectoriser = TfidfVectorizer(stop_words=\"english\")\n",
    "syn_merge_X = syn_merge_vectoriser.fit_transform(df[\"clean_docs\"])\n",
    "len(syn_merge_vectoriser.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840afc6a",
   "metadata": {},
   "source": [
    "Simply taking the first option and removing the extra information in the synset object name (f.ex. \"skin.n.01\"), does reduce the dimensionality, but it doesn't take into account the context the word appeared in. Let's see how much more of a difference can we get by removing words that only occur once in the whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df.copy()\n",
    "df[\"clean_docs\"] = df[\"description\"].str.replace(r\"[^a-zA-Z0-9']+\", \" \", regex=True)\\\n",
    "                                    .apply(lambda desc: nlp(desc.lower()))\\\n",
    "                                    .apply(lambda doc: [token.lemma_ for token in doc\n",
    "                                                        if token.text not in STOP_WORDS])\\\n",
    "                                    .apply(lambda ls: \" \".join(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df066401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Active classic boxers - There's a reason why o...</td>\n",
       "      <td>active classic boxer reason boxer cult favorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n",
       "      <td>active sport boxer brief skin glory require mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Active sport briefs - These superbreathable no...</td>\n",
       "      <td>active sport brief superbreathable fly brief m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n",
       "      <td>alpine guide pant skin climb ice switch rock t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n",
       "      <td>alpine wind jkt high ridge steep ice alpine ja...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        description  \\\n",
       "0   1  Active classic boxers - There's a reason why o...   \n",
       "1   2  Active sport boxer briefs - Skinning up Glory ...   \n",
       "2   3  Active sport briefs - These superbreathable no...   \n",
       "3   4  Alpine guide pants - Skin in, climb ice, switc...   \n",
       "4   5  Alpine wind jkt - On high ridges, steep ice an...   \n",
       "\n",
       "                                          clean_docs  \n",
       "0  active classic boxer reason boxer cult favorit...  \n",
       "1  active sport boxer brief skin glory require mo...  \n",
       "2  active sport brief superbreathable fly brief m...  \n",
       "3  alpine guide pant skin climb ice switch rock t...  \n",
       "4  alpine wind jkt high ridge steep ice alpine ja...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328628a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectoriser = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectoriser.fit_transform(df[\"clean_docs\"])\n",
    "len(vectoriser.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d348b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active</th>\n",
       "      <th>classic</th>\n",
       "      <th>boxer</th>\n",
       "      <th>reason</th>\n",
       "      <th>cult</th>\n",
       "      <th>favorite</th>\n",
       "      <th>cool</th>\n",
       "      <th>especially</th>\n",
       "      <th>sticky</th>\n",
       "      <th>situation</th>\n",
       "      <th>...</th>\n",
       "      <th>493</th>\n",
       "      <th>paint</th>\n",
       "      <th>splatter</th>\n",
       "      <th>cake</th>\n",
       "      <th>washing</th>\n",
       "      <th>bellow</th>\n",
       "      <th>349</th>\n",
       "      <th>arduous</th>\n",
       "      <th>unfazed</th>\n",
       "      <th>282</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100574</td>\n",
       "      <td>0.146854</td>\n",
       "      <td>0.103911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047799</td>\n",
       "      <td>0.047159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       active  classic     boxer    reason  cult  favorite  cool  especially  \\\n",
       "doc_1     0.0      0.0  0.000000  0.073055   0.0       0.0   0.0         0.0   \n",
       "doc_2     0.0      0.0  0.000000  0.000000   0.0       0.0   0.0         0.0   \n",
       "doc_3     0.0      0.0  0.000000  0.000000   0.0       0.0   0.0         0.0   \n",
       "doc_4     0.0      0.0  0.000000  0.000000   0.0       0.0   0.0         0.0   \n",
       "doc_5     0.0      0.0  0.047799  0.047159   0.0       0.0   0.0         0.0   \n",
       "\n",
       "       sticky  situation  ...  493  paint  splatter  cake  washing    bellow  \\\n",
       "doc_1     0.0        0.0  ...  0.0    0.0       0.0   0.0      0.0  0.000000   \n",
       "doc_2     0.0        0.0  ...  0.0    0.0       0.0   0.0      0.0  0.000000   \n",
       "doc_3     0.0        0.0  ...  0.0    0.0       0.0   0.0      0.0  0.000000   \n",
       "doc_4     0.0        0.0  ...  0.0    0.0       0.0   0.0      0.0  0.100574   \n",
       "doc_5     0.0        0.0  ...  0.0    0.0       0.0   0.0      0.0  0.151113   \n",
       "\n",
       "            349   arduous  unfazed  282  \n",
       "doc_1  0.000000  0.000000      0.0  0.0  \n",
       "doc_2  0.000000  0.000000      0.0  0.0  \n",
       "doc_3  0.000000  0.000000      0.0  0.0  \n",
       "doc_4  0.146854  0.103911      0.0  0.0  \n",
       "doc_5  0.000000  0.000000      0.0  0.0  \n",
       "\n",
       "[5 rows x 3825 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense = X.todense()\n",
    "tfidf_df = pd.DataFrame(dense, \n",
    "                        columns=vectoriser.vocabulary_, \n",
    "                        index=[f\"doc_{x}\" for x in range(1, dense.shape[0]+1)])\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab15719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len([term for term in vectoriser.vocabulary_\n",
    "     if len(tfidf_df[tfidf_df[term] != 0.0]) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cce0f9",
   "metadata": {},
   "source": [
    "Removing unique lemmas is a lot better. \\\n",
    "Just out of curiosity, let's see how much picking the first synonym helped with unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4998ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_agent</th>\n",
       "      <th>classic</th>\n",
       "      <th>boxer</th>\n",
       "      <th>reason</th>\n",
       "      <th>cult</th>\n",
       "      <th>favorite</th>\n",
       "      <th>cool</th>\n",
       "      <th>particularly</th>\n",
       "      <th>gluey</th>\n",
       "      <th>situation</th>\n",
       "      <th>...</th>\n",
       "      <th>jeer</th>\n",
       "      <th>493</th>\n",
       "      <th>paint</th>\n",
       "      <th>spatter</th>\n",
       "      <th>cake</th>\n",
       "      <th>bellow</th>\n",
       "      <th>349</th>\n",
       "      <th>arduous</th>\n",
       "      <th>unfazed</th>\n",
       "      <th>282</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       active_agent  classic  boxer  reason  cult  favorite  cool  \\\n",
       "doc_1           0.0      0.0    0.0     0.0   0.0       0.0   0.0   \n",
       "doc_2           0.0      0.0    0.0     0.0   0.0       0.0   0.0   \n",
       "doc_3           0.0      0.0    0.0     0.0   0.0       0.0   0.0   \n",
       "doc_4           0.0      0.0    0.0     0.0   0.0       0.0   0.0   \n",
       "doc_5           0.0      0.0    0.0     0.0   0.0       0.0   0.0   \n",
       "\n",
       "       particularly  gluey  situation  ...  jeer  493  paint  spatter  cake  \\\n",
       "doc_1           0.0    0.0        0.0  ...   0.0  0.0    0.0      0.0   0.0   \n",
       "doc_2           0.0    0.0        0.0  ...   0.0  0.0    0.0      0.0   0.0   \n",
       "doc_3           0.0    0.0        0.0  ...   0.0  0.0    0.0      0.0   0.0   \n",
       "doc_4           0.0    0.0        0.0  ...   0.0  0.0    0.0      0.0   0.0   \n",
       "doc_5           0.0    0.0        0.0  ...   0.0  0.0    0.0      0.0   0.0   \n",
       "\n",
       "       bellow  349  arduous  unfazed  282  \n",
       "doc_1     0.0  0.0      0.0  0.00000  0.0  \n",
       "doc_2     0.0  0.0      0.0  0.00000  0.0  \n",
       "doc_3     0.0  0.0      0.0  0.00000  0.0  \n",
       "doc_4     0.0  0.0      0.0  0.14985  0.0  \n",
       "doc_5     0.0  0.0      0.0  0.00000  0.0  \n",
       "\n",
       "[5 rows x 3434 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_merge_dense = syn_merge_X.todense()\n",
    "syn_merge_tfidf_df = pd.DataFrame(syn_merge_dense, \n",
    "                                  columns=syn_merge_vectoriser.vocabulary_, \n",
    "                                  index=[f\"doc_{x}\" for x in range(1, syn_merge_dense.shape[0]+1)] )\n",
    "syn_merge_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2033"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len([term for term in syn_merge_vectoriser.vocabulary_\n",
    "     if len(syn_merge_tfidf_df[syn_merge_tfidf_df[term] != 0.0]) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93742817",
   "metadata": {},
   "source": [
    "It seems like it did reduce the proportion of unique words a little. But at what cost?\\\n",
    "With more time, I may have been able to find a way to merge synonyms, but the risk of merging words with different meanings and significantly reducing the quality of the data is too big, and removing unique features makes a big difference already, so I won't pursue the synonym pre-processing step further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc6e67",
   "metadata": {},
   "source": [
    "## Improving the Set of Features in Other Ways\n",
    "\n",
    "The idea now is to run TfidfVectorizer on the data with 1- to 4-grams to reveal more patterns, and then remove any feature that is unique in the TF-IDF matrix (n-gram or simple lemma). Features that only have one non-zero value on one doc cannot help show common patterns between docs.\n",
    "I also realised that I had left the HTML tags in at that stage. That gets fixed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f938d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df.copy()\n",
    "df[\"clean_docs\"] = df[\"description\"].str.replace(r\"<[^>]*>\", \" \", regex=True)\\\n",
    "                                    .str.replace(r\"[^a-zA-Z0-9']+\", \" \", regex=True)\\\n",
    "                                    .apply(lambda desc: nlp(desc.lower()))\\\n",
    "                                    .apply(lambda doc: [token.lemma_ for token in doc\n",
    "                                                        if token.text not in STOP_WORDS])\\\n",
    "                                    .apply(lambda ls: \" \".join(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe360fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80511"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I am not using a max or min document frequency for terms because I want to decide which cols to keep after the n-grams are made\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 4))\n",
    "X = vectoriser.fit_transform(df[\"clean_docs\"])\n",
    "len(vectoriser.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af831820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active</th>\n",
       "      <th>classic</th>\n",
       "      <th>boxer</th>\n",
       "      <th>reason</th>\n",
       "      <th>cult</th>\n",
       "      <th>favorite</th>\n",
       "      <th>cool</th>\n",
       "      <th>especially</th>\n",
       "      <th>sticky</th>\n",
       "      <th>situation</th>\n",
       "      <th>...</th>\n",
       "      <th>flat zip fly button</th>\n",
       "      <th>entry drop pocket welt</th>\n",
       "      <th>drop pocket welt pocket</th>\n",
       "      <th>welt pocket inseam update</th>\n",
       "      <th>pocket inseam update fit</th>\n",
       "      <th>inseam update fit fabric</th>\n",
       "      <th>update fit fabric oz</th>\n",
       "      <th>recycle program weight 282</th>\n",
       "      <th>program weight 282 oz</th>\n",
       "      <th>weight 282 oz thailand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80511 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       active  classic  boxer  reason  cult  favorite  cool  especially  \\\n",
       "doc_1     0.0      0.0    0.0     0.0   0.0       0.0   0.0         0.0   \n",
       "doc_2     0.0      0.0    0.0     0.0   0.0       0.0   0.0         0.0   \n",
       "doc_3     0.0      0.0    0.0     0.0   0.0       0.0   0.0         0.0   \n",
       "doc_4     0.0      0.0    0.0     0.0   0.0       0.0   0.0         0.0   \n",
       "doc_5     0.0      0.0    0.0     0.0   0.0       0.0   0.0         0.0   \n",
       "\n",
       "       sticky  situation  ...  flat zip fly button  entry drop pocket welt  \\\n",
       "doc_1     0.0        0.0  ...                  0.0                     0.0   \n",
       "doc_2     0.0        0.0  ...                  0.0                     0.0   \n",
       "doc_3     0.0        0.0  ...                  0.0                     0.0   \n",
       "doc_4     0.0        0.0  ...                  0.0                     0.0   \n",
       "doc_5     0.0        0.0  ...                  0.0                     0.0   \n",
       "\n",
       "       drop pocket welt pocket  welt pocket inseam update  \\\n",
       "doc_1                      0.0                        0.0   \n",
       "doc_2                      0.0                        0.0   \n",
       "doc_3                      0.0                        0.0   \n",
       "doc_4                      0.0                        0.0   \n",
       "doc_5                      0.0                        0.0   \n",
       "\n",
       "       pocket inseam update fit  inseam update fit fabric  \\\n",
       "doc_1                       0.0                       0.0   \n",
       "doc_2                       0.0                       0.0   \n",
       "doc_3                       0.0                       0.0   \n",
       "doc_4                       0.0                       0.0   \n",
       "doc_5                       0.0                       0.0   \n",
       "\n",
       "       update fit fabric oz  recycle program weight 282  \\\n",
       "doc_1                   0.0                         0.0   \n",
       "doc_2                   0.0                         0.0   \n",
       "doc_3                   0.0                         0.0   \n",
       "doc_4                   0.0                         0.0   \n",
       "doc_5                   0.0                         0.0   \n",
       "\n",
       "       program weight 282 oz  weight 282 oz thailand  \n",
       "doc_1                    0.0                     0.0  \n",
       "doc_2                    0.0                     0.0  \n",
       "doc_3                    0.0                     0.0  \n",
       "doc_4                    0.0                     0.0  \n",
       "doc_5                    0.0                     0.0  \n",
       "\n",
       "[5 rows x 80511 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense = X.todense()\n",
    "tfidf_df = pd.DataFrame(dense, \n",
    "                        columns=vectoriser.vocabulary_, \n",
    "                        index=[f\"doc_{x}\" for x in range(1, dense.shape[0]+1)])\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active</th>\n",
       "      <th>classic</th>\n",
       "      <th>boxer</th>\n",
       "      <th>reason</th>\n",
       "      <th>lightweight</th>\n",
       "      <th>travel</th>\n",
       "      <th>pack</th>\n",
       "      <th>expose</th>\n",
       "      <th>softness</th>\n",
       "      <th>traditional</th>\n",
       "      <th>...</th>\n",
       "      <th>flat zip fly button</th>\n",
       "      <th>entry drop pocket welt</th>\n",
       "      <th>drop pocket welt pocket</th>\n",
       "      <th>welt pocket inseam update</th>\n",
       "      <th>pocket inseam update fit</th>\n",
       "      <th>inseam update fit fabric</th>\n",
       "      <th>update fit fabric oz</th>\n",
       "      <th>recycle program weight 282</th>\n",
       "      <th>program weight 282 oz</th>\n",
       "      <th>weight 282 oz thailand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       active  classic  boxer  reason  lightweight  travel  pack  expose  \\\n",
       "doc_1     0.0      0.0    0.0     0.0     0.000000     0.0   0.0     0.0   \n",
       "doc_2     0.0      0.0    0.0     0.0     0.000000     0.0   0.0     0.0   \n",
       "doc_3     0.0      0.0    0.0     0.0     0.000000     0.0   0.0     0.0   \n",
       "doc_4     0.0      0.0    0.0     0.0     0.000000     0.0   0.0     0.0   \n",
       "doc_5     0.0      0.0    0.0     0.0     0.019869     0.0   0.0     0.0   \n",
       "\n",
       "       softness  traditional  ...  flat zip fly button  \\\n",
       "doc_1       0.0          0.0  ...                  0.0   \n",
       "doc_2       0.0          0.0  ...                  0.0   \n",
       "doc_3       0.0          0.0  ...                  0.0   \n",
       "doc_4       0.0          0.0  ...                  0.0   \n",
       "doc_5       0.0          0.0  ...                  0.0   \n",
       "\n",
       "       entry drop pocket welt  drop pocket welt pocket  \\\n",
       "doc_1                     0.0                      0.0   \n",
       "doc_2                     0.0                      0.0   \n",
       "doc_3                     0.0                      0.0   \n",
       "doc_4                     0.0                      0.0   \n",
       "doc_5                     0.0                      0.0   \n",
       "\n",
       "       welt pocket inseam update  pocket inseam update fit  \\\n",
       "doc_1                        0.0                       0.0   \n",
       "doc_2                        0.0                       0.0   \n",
       "doc_3                        0.0                       0.0   \n",
       "doc_4                        0.0                       0.0   \n",
       "doc_5                        0.0                       0.0   \n",
       "\n",
       "       inseam update fit fabric  update fit fabric oz  \\\n",
       "doc_1                       0.0                   0.0   \n",
       "doc_2                       0.0                   0.0   \n",
       "doc_3                       0.0                   0.0   \n",
       "doc_4                       0.0                   0.0   \n",
       "doc_5                       0.0                   0.0   \n",
       "\n",
       "       recycle program weight 282  program weight 282 oz  \\\n",
       "doc_1                         0.0                    0.0   \n",
       "doc_2                         0.0                    0.0   \n",
       "doc_3                         0.0                    0.0   \n",
       "doc_4                         0.0                    0.0   \n",
       "doc_5                         0.0                    0.0   \n",
       "\n",
       "       weight 282 oz thailand  \n",
       "doc_1                     0.0  \n",
       "doc_2                     0.0  \n",
       "doc_3                     0.0  \n",
       "doc_4                     0.0  \n",
       "doc_5                     0.0  \n",
       "\n",
       "[5 rows x 23757 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_more_than_one = [term for term in vectoriser.vocabulary_\n",
    "                      if len(tfidf_df[tfidf_df[term] != 0.0]) > 1] # I checked that no elements were negative, but I used `!=` just to be sure.\n",
    "denser_df = tfidf_df[cols_more_than_one]\n",
    "denser_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab6584",
   "metadata": {},
   "source": [
    "Our lessons say that if there are about 1/5 non-zero values in the TF-IDF matrix, I should try to reduce the number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beaa0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 499, 499, 499], shape=(120046,)),\n",
       " array([   35,   132,   137, ..., 23352, 23357, 23358], shape=(120046,)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "denser_df.to_numpy().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc119c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0106158184956013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(120046 / (500 * 23757)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16284a",
   "metadata": {},
   "source": [
    "There is only 1% of non-zero values! Getting it to 20% is going to be hard, but let's see what it would take.\n",
    "\n",
    "Let's see if I can remove features with a low average TF-IDF (features that are comparatively less useful to extract topics that stand out) before performing a truncated SVD. Although this may remove very common words first and make the situation worse, I want to see if we can find a sweet spot or if it's impossible.\n",
    "\n",
    "Then, I'll see how many columns I would need to remove to reach 20%. It seems a lot more doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9442/80511 terms with an average TF-IDF above 0.051000000000000004, with 0.7570853632704935% non-zero values.\n",
      "There are 8720/80511 terms with an average TF-IDF above 0.052000000000000005, with 0.7638302752293578% non-zero values.\n",
      "There are 8255/80511 terms with an average TF-IDF above 0.053000000000000005, with 0.767244094488189% non-zero values.\n",
      "There are 7619/80511 terms with an average TF-IDF above 0.054000000000000006, with 0.7735135844599028% non-zero values.\n",
      "There are 7182/80511 terms with an average TF-IDF above 0.05500000000000001, with 0.7752715121136173% non-zero values.\n",
      "There are 6283/80511 terms with an average TF-IDF above 0.05600000000000001, with 0.7828425911188923% non-zero values.\n",
      "There are 5915/80511 terms with an average TF-IDF above 0.05700000000000001, with 0.7761284868977177% non-zero values.\n",
      "There are 5360/80511 terms with an average TF-IDF above 0.05800000000000001, with 0.7747014925373134% non-zero values.\n",
      "There are 5131/80511 terms with an average TF-IDF above 0.05900000000000001, with 0.7658156304813877% non-zero values.\n",
      "There are 4862/80511 terms with an average TF-IDF above 0.06000000000000001, with 0.7380501851090087% non-zero values.\n",
      "There are 4387/80511 terms with an average TF-IDF above 0.06100000000000001, with 0.7281057670389788% non-zero values.\n",
      "There are 4198/80511 terms with an average TF-IDF above 0.06200000000000001, with 0.7267746545974274% non-zero values.\n",
      "There are 3891/80511 terms with an average TF-IDF above 0.06300000000000001, with 0.7258802364430738% non-zero values.\n",
      "There are 3620/80511 terms with an average TF-IDF above 0.06400000000000002, with 0.7276795580110497% non-zero values.\n",
      "There are 3506/80511 terms with an average TF-IDF above 0.06500000000000002, with 0.7156303479749002% non-zero values.\n",
      "There are 3362/80511 terms with an average TF-IDF above 0.06600000000000002, with 0.7035098155859607% non-zero values.\n",
      "There are 3231/80511 terms with an average TF-IDF above 0.06700000000000002, with 0.6815846487155679% non-zero values.\n",
      "There are 2963/80511 terms with an average TF-IDF above 0.06800000000000002, with 0.6608842389470132% non-zero values.\n",
      "There are 2759/80511 terms with an average TF-IDF above 0.06900000000000002, with 0.6651685393258426% non-zero values.\n",
      "There are 2516/80511 terms with an average TF-IDF above 0.07000000000000002, with 0.6746422893481717% non-zero values.\n",
      "There are 2430/80511 terms with an average TF-IDF above 0.07100000000000002, with 0.6692181069958848% non-zero values.\n",
      "There are 2253/80511 terms with an average TF-IDF above 0.07200000000000002, with 0.6776742121615623% non-zero values.\n",
      "There are 2163/80511 terms with an average TF-IDF above 0.07300000000000002, with 0.6781322237632917% non-zero values.\n",
      "There are 2082/80511 terms with an average TF-IDF above 0.07400000000000002, with 0.6720461095100865% non-zero values.\n",
      "There are 2018/80511 terms with an average TF-IDF above 0.07500000000000002, with 0.6619425173439049% non-zero values.\n",
      "There are 1734/80511 terms with an average TF-IDF above 0.07600000000000003, with 0.6907727797001153% non-zero values.\n",
      "There are 1600/80511 terms with an average TF-IDF above 0.07700000000000003, with 0.69275% non-zero values.\n",
      "There are 1555/80511 terms with an average TF-IDF above 0.07800000000000003, with 0.689903536977492% non-zero values.\n",
      "There are 1475/80511 terms with an average TF-IDF above 0.07900000000000003, with 0.6980338983050848% non-zero values.\n",
      "There are 1381/80511 terms with an average TF-IDF above 0.08000000000000003, with 0.7002172338884867% non-zero values.\n",
      "There are 1332/80511 terms with an average TF-IDF above 0.08100000000000003, with 0.7012012012012012% non-zero values.\n",
      "There are 1255/80511 terms with an average TF-IDF above 0.08200000000000003, with 0.7078884462151394% non-zero values.\n",
      "There are 1167/80511 terms with an average TF-IDF above 0.08300000000000003, with 0.7017994858611826% non-zero values.\n",
      "There are 1137/80511 terms with an average TF-IDF above 0.08400000000000003, with 0.698504837291117% non-zero values.\n",
      "There are 975/80511 terms with an average TF-IDF above 0.08500000000000003, with 0.7144615384615385% non-zero values.\n",
      "There are 937/80511 terms with an average TF-IDF above 0.08600000000000003, with 0.7107790821771611% non-zero values.\n",
      "There are 907/80511 terms with an average TF-IDF above 0.08700000000000004, with 0.7078280044101434% non-zero values.\n",
      "There are 880/80511 terms with an average TF-IDF above 0.08800000000000004, with 0.6977272727272726% non-zero values.\n",
      "There are 823/80511 terms with an average TF-IDF above 0.08900000000000004, with 0.7025516403402188% non-zero values.\n",
      "There are 804/80511 terms with an average TF-IDF above 0.09000000000000004, with 0.7034825870646766% non-zero values.\n",
      "There are 725/80511 terms with an average TF-IDF above 0.09100000000000004, with 0.720551724137931% non-zero values.\n",
      "There are 674/80511 terms with an average TF-IDF above 0.09200000000000004, with 0.7261127596439169% non-zero values.\n",
      "There are 642/80511 terms with an average TF-IDF above 0.09300000000000004, with 0.702803738317757% non-zero values.\n",
      "There are 553/80511 terms with an average TF-IDF above 0.09400000000000004, with 0.7291139240506329% non-zero values.\n",
      "There are 531/80511 terms with an average TF-IDF above 0.09500000000000004, with 0.7322033898305085% non-zero values.\n",
      "There are 501/80511 terms with an average TF-IDF above 0.09600000000000004, with 0.7305389221556886% non-zero values.\n",
      "There are 419/80511 terms with an average TF-IDF above 0.09700000000000004, with 0.7727923627684964% non-zero values.\n",
      "There are 401/80511 terms with an average TF-IDF above 0.09800000000000005, with 0.773067331670823% non-zero values.\n",
      "There are 387/80511 terms with an average TF-IDF above 0.09900000000000005, with 0.7777777777777778% non-zero values.\n",
      "There are 378/80511 terms with an average TF-IDF above 0.10000000000000005, with 0.771957671957672% non-zero values.\n",
      "There are 330/80511 terms with an average TF-IDF above 0.10100000000000005, with 0.7290909090909091% non-zero values.\n",
      "There are 324/80511 terms with an average TF-IDF above 0.10200000000000005, with 0.7351851851851852% non-zero values.\n",
      "There are 318/80511 terms with an average TF-IDF above 0.10300000000000005, with 0.7364779874213837% non-zero values.\n",
      "There are 300/80511 terms with an average TF-IDF above 0.10400000000000005, with 0.738% non-zero values.\n",
      "There are 289/80511 terms with an average TF-IDF above 0.10500000000000005, with 0.7439446366782008% non-zero values.\n",
      "There are 270/80511 terms with an average TF-IDF above 0.10600000000000005, with 0.7429629629629629% non-zero values.\n",
      "There are 254/80511 terms with an average TF-IDF above 0.10700000000000005, with 0.7590551181102362% non-zero values.\n",
      "There are 237/80511 terms with an average TF-IDF above 0.10800000000000005, with 0.6641350210970464% non-zero values.\n",
      "There are 215/80511 terms with an average TF-IDF above 0.10900000000000006, with 0.6744186046511628% non-zero values.\n",
      "There are 207/80511 terms with an average TF-IDF above 0.11000000000000006, with 0.6792270531400966% non-zero values.\n",
      "There are 200/80511 terms with an average TF-IDF above 0.11100000000000006, with 0.687% non-zero values.\n",
      "There are 181/80511 terms with an average TF-IDF above 0.11200000000000006, with 0.7049723756906078% non-zero values.\n",
      "There are 175/80511 terms with an average TF-IDF above 0.11300000000000006, with 0.6971428571428571% non-zero values.\n",
      "There are 149/80511 terms with an average TF-IDF above 0.11400000000000006, with 0.6859060402684564% non-zero values.\n",
      "There are 143/80511 terms with an average TF-IDF above 0.11500000000000006, with 0.6727272727272727% non-zero values.\n",
      "There are 143/80511 terms with an average TF-IDF above 0.11600000000000006, with 0.6727272727272727% non-zero values.\n",
      "There are 138/80511 terms with an average TF-IDF above 0.11700000000000006, with 0.6811594202898551% non-zero values.\n",
      "There are 136/80511 terms with an average TF-IDF above 0.11800000000000006, with 0.6823529411764706% non-zero values.\n",
      "There are 129/80511 terms with an average TF-IDF above 0.11900000000000006, with 0.6713178294573643% non-zero values.\n",
      "There are 127/80511 terms with an average TF-IDF above 0.12000000000000006, with 0.6519685039370079% non-zero values.\n",
      "There are 118/80511 terms with an average TF-IDF above 0.12100000000000007, with 0.6661016949152543% non-zero values.\n",
      "There are 113/80511 terms with an average TF-IDF above 0.12200000000000007, with 0.6690265486725664% non-zero values.\n",
      "There are 113/80511 terms with an average TF-IDF above 0.12300000000000007, with 0.6690265486725664% non-zero values.\n",
      "There are 111/80511 terms with an average TF-IDF above 0.12400000000000007, with 0.6702702702702703% non-zero values.\n",
      "There are 101/80511 terms with an average TF-IDF above 0.12500000000000006, with 0.6495049504950495% non-zero values.\n",
      "There are 99/80511 terms with an average TF-IDF above 0.12600000000000006, with 0.6505050505050505% non-zero values.\n",
      "There are 95/80511 terms with an average TF-IDF above 0.12700000000000006, with 0.6610526315789473% non-zero values.\n",
      "There are 92/80511 terms with an average TF-IDF above 0.12800000000000006, with 0.6652173913043479% non-zero values.\n",
      "There are 92/80511 terms with an average TF-IDF above 0.12900000000000006, with 0.6652173913043479% non-zero values.\n",
      "There are 86/80511 terms with an average TF-IDF above 0.13000000000000006, with 0.6813953488372093% non-zero values.\n",
      "There are 86/80511 terms with an average TF-IDF above 0.13100000000000006, with 0.6813953488372093% non-zero values.\n",
      "There are 79/80511 terms with an average TF-IDF above 0.13200000000000006, with 0.6734177215189874% non-zero values.\n",
      "There are 77/80511 terms with an average TF-IDF above 0.13300000000000006, with 0.6805194805194805% non-zero values.\n",
      "There are 72/80511 terms with an average TF-IDF above 0.13400000000000006, with 0.6722222222222223% non-zero values.\n",
      "There are 70/80511 terms with an average TF-IDF above 0.13500000000000006, with 0.6628571428571429% non-zero values.\n",
      "There are 69/80511 terms with an average TF-IDF above 0.13600000000000007, with 0.6608695652173914% non-zero values.\n",
      "There are 59/80511 terms with an average TF-IDF above 0.13700000000000007, with 0.7016949152542372% non-zero values.\n",
      "There are 53/80511 terms with an average TF-IDF above 0.13800000000000007, with 0.7056603773584906% non-zero values.\n",
      "There are 52/80511 terms with an average TF-IDF above 0.13900000000000007, with 0.7076923076923077% non-zero values.\n",
      "There are 48/80511 terms with an average TF-IDF above 0.14000000000000007, with 0.7208333333333333% non-zero values.\n",
      "There are 48/80511 terms with an average TF-IDF above 0.14100000000000007, with 0.7208333333333333% non-zero values.\n",
      "There are 48/80511 terms with an average TF-IDF above 0.14200000000000007, with 0.7208333333333333% non-zero values.\n",
      "There are 46/80511 terms with an average TF-IDF above 0.14300000000000007, with 0.7086956521739131% non-zero values.\n",
      "There are 41/80511 terms with an average TF-IDF above 0.14400000000000007, with 0.7463414634146341% non-zero values.\n",
      "There are 39/80511 terms with an average TF-IDF above 0.14500000000000007, with 0.764102564102564% non-zero values.\n",
      "There are 38/80511 terms with an average TF-IDF above 0.14600000000000007, with 0.731578947368421% non-zero values.\n",
      "There are 38/80511 terms with an average TF-IDF above 0.14700000000000008, with 0.731578947368421% non-zero values.\n",
      "There are 38/80511 terms with an average TF-IDF above 0.14800000000000008, with 0.731578947368421% non-zero values.\n",
      "There are 36/80511 terms with an average TF-IDF above 0.14900000000000008, with 0.7333333333333333% non-zero values.\n",
      "There are 34/80511 terms with an average TF-IDF above 0.15000000000000008, with 0.711764705882353% non-zero values.\n",
      "There are 28/80511 terms with an average TF-IDF above 0.15100000000000008, with 0.7714285714285715% non-zero values.\n",
      "There are 22/80511 terms with an average TF-IDF above 0.15200000000000008, with 0.8727272727272728% non-zero values.\n",
      "There are 22/80511 terms with an average TF-IDF above 0.15300000000000008, with 0.8727272727272728% non-zero values.\n",
      "There are 22/80511 terms with an average TF-IDF above 0.15400000000000008, with 0.8727272727272728% non-zero values.\n",
      "There are 22/80511 terms with an average TF-IDF above 0.15500000000000008, with 0.8727272727272728% non-zero values.\n",
      "There are 21/80511 terms with an average TF-IDF above 0.15600000000000008, with 0.819047619047619% non-zero values.\n",
      "There are 20/80511 terms with an average TF-IDF above 0.15700000000000008, with 0.62% non-zero values.\n",
      "There are 18/80511 terms with an average TF-IDF above 0.15800000000000008, with 0.6222222222222222% non-zero values.\n",
      "There are 18/80511 terms with an average TF-IDF above 0.15900000000000009, with 0.6222222222222222% non-zero values.\n",
      "There are 17/80511 terms with an average TF-IDF above 0.1600000000000001, with 0.611764705882353% non-zero values.\n",
      "There are 17/80511 terms with an average TF-IDF above 0.1610000000000001, with 0.611764705882353% non-zero values.\n",
      "There are 17/80511 terms with an average TF-IDF above 0.1620000000000001, with 0.611764705882353% non-zero values.\n",
      "There are 16/80511 terms with an average TF-IDF above 0.1630000000000001, with 0.625% non-zero values.\n",
      "There are 16/80511 terms with an average TF-IDF above 0.1640000000000001, with 0.625% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1650000000000001, with 0.64% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1660000000000001, with 0.64% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1670000000000001, with 0.64% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1680000000000001, with 0.64% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1690000000000001, with 0.64% non-zero values.\n",
      "There are 15/80511 terms with an average TF-IDF above 0.1700000000000001, with 0.64% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1710000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1720000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1730000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1740000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1750000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1760000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1770000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1780000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1790000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1800000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1810000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1820000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 14/80511 terms with an average TF-IDF above 0.1830000000000001, with 0.5857142857142856% non-zero values.\n",
      "There are 11/80511 terms with an average TF-IDF above 0.1840000000000001, with 0.6363636363636364% non-zero values.\n",
      "There are 11/80511 terms with an average TF-IDF above 0.1850000000000001, with 0.6363636363636364% non-zero values.\n",
      "There are 11/80511 terms with an average TF-IDF above 0.1860000000000001, with 0.6363636363636364% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.1870000000000001, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.1880000000000001, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.1890000000000001, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.1900000000000001, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.19100000000000011, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.19200000000000012, with 0.7714285714285715% non-zero values.\n",
      "There are 7/80511 terms with an average TF-IDF above 0.19300000000000012, with 0.7714285714285715% non-zero values.\n",
      "There are 5/80511 terms with an average TF-IDF above 0.19400000000000012, with 0.9199999999999999% non-zero values.\n",
      "There are 5/80511 terms with an average TF-IDF above 0.19500000000000012, with 0.9199999999999999% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.19600000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.19700000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.19800000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.19900000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.20000000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.20100000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.20200000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 3/80511 terms with an average TF-IDF above 0.20300000000000012, with 0.7333333333333333% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20400000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20500000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20600000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20700000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20800000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.20900000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21000000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21100000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21200000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21300000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21400000000000013, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21500000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21600000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21700000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21800000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.21900000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.22000000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 2/80511 terms with an average TF-IDF above 0.22100000000000014, with 0.8999999999999999% non-zero values.\n",
      "There are 1/80511 terms with an average TF-IDF above 0.22200000000000014, with 0.8% non-zero values.\n"
     ]
    }
   ],
   "source": [
    "# Removing terms with low average TF-IDF\n",
    "\n",
    "percentage_non_zeros = -1\n",
    "num_features = np.inf\n",
    "threshold = 0.05\n",
    "full_num_terms = len(tfidf_df.columns)\n",
    "eligible_features = [term for term in denser_df.columns\n",
    "                    if (denser_df[denser_df[term] > 0][term].mean() > threshold)]\n",
    "current_df = denser_df[eligible_features]\n",
    "\n",
    "while (percentage_non_zeros < 20) and num_features > 1:\n",
    "    threshold += 0.001\n",
    "    eligible_features = [term for term in current_df.columns\n",
    "                        if (current_df[current_df[term] > 0][term].mean() > threshold)]\n",
    "    current_df = current_df[eligible_features]\n",
    "    num_features = len(eligible_features)\n",
    "    percentage_non_zeros = (\n",
    "        len(current_df.to_numpy().nonzero()[0])\n",
    "        / (500 * num_features)\n",
    "        ) * 100\n",
    "    print(f\"There are {num_features}/{full_num_terms} terms with an average TF-IDF above {threshold}, with {percentage_non_zeros}% non-zero values.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd864bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10717/23757 terms that occur more than 2 times in the TF-IDF matrix, with 1.753587757768032% non-zero values.\n",
      "There are 7333/23757 terms that occur more than 3 times in the TF-IDF matrix, with 2.285940270012273% non-zero values.\n",
      "There are 5035/23757 terms that occur more than 4 times in the TF-IDF matrix, with 2.9641310824230387% non-zero values.\n",
      "There are 4002/23757 terms that occur more than 5 times in the TF-IDF matrix, with 3.4711144427786103% non-zero values.\n",
      "There are 3047/23757 terms that occur more than 6 times in the TF-IDF matrix, with 4.18293403347555% non-zero values.\n",
      "There are 2471/23757 terms that occur more than 7 times in the TF-IDF matrix, with 4.831647106434642% non-zero values.\n",
      "There are 1959/23757 terms that occur more than 8 times in the TF-IDF matrix, with 5.676263399693721% non-zero values.\n",
      "There are 1614/23757 terms that occur more than 9 times in the TF-IDF matrix, with 6.504832713754646% non-zero values.\n",
      "There are 1425/23757 terms that occur more than 10 times in the TF-IDF matrix, with 7.102315789473685% non-zero values.\n",
      "There are 1279/23757 terms that occur more than 11 times in the TF-IDF matrix, with 7.66192337763878% non-zero values.\n",
      "There are 1176/23757 terms that occur more than 12 times in the TF-IDF matrix, with 8.12278911564626% non-zero values.\n",
      "There are 1108/23757 terms that occur more than 13 times in the TF-IDF matrix, with 8.461732851985559% non-zero values.\n",
      "There are 992/23757 terms that occur more than 14 times in the TF-IDF matrix, with 9.123790322580644% non-zero values.\n",
      "There are 844/23757 terms that occur more than 15 times in the TF-IDF matrix, with 10.197630331753555% non-zero values.\n",
      "There are 791/23757 terms that occur more than 16 times in the TF-IDF matrix, with 10.666498103666246% non-zero values.\n",
      "There are 728/23757 terms that occur more than 17 times in the TF-IDF matrix, with 11.295329670329672% non-zero values.\n",
      "There are 686/23757 terms that occur more than 18 times in the TF-IDF matrix, with 11.766472303206998% non-zero values.\n",
      "There are 668/23757 terms that occur more than 19 times in the TF-IDF matrix, with 11.981137724550898% non-zero values.\n",
      "There are 619/23757 terms that occur more than 20 times in the TF-IDF matrix, with 12.612924071082391% non-zero values.\n",
      "There are 582/23757 terms that occur more than 21 times in the TF-IDF matrix, with 13.147766323024054% non-zero values.\n",
      "There are 551/23757 terms that occur more than 22 times in the TF-IDF matrix, with 13.639927404718694% non-zero values.\n",
      "There are 519/23757 terms that occur more than 23 times in the TF-IDF matrix, with 14.197302504816955% non-zero values.\n",
      "There are 493/23757 terms that occur more than 24 times in the TF-IDF matrix, with 14.692900608519269% non-zero values.\n",
      "There are 480/23757 terms that occur more than 25 times in the TF-IDF matrix, with 14.955416666666665% non-zero values.\n",
      "There are 462/23757 terms that occur more than 26 times in the TF-IDF matrix, with 15.335497835497836% non-zero values.\n",
      "There are 446/23757 terms that occur more than 27 times in the TF-IDF matrix, with 15.691928251121077% non-zero values.\n",
      "There are 435/23757 terms that occur more than 28 times in the TF-IDF matrix, with 15.947126436781609% non-zero values.\n",
      "There are 417/23757 terms that occur more than 29 times in the TF-IDF matrix, with 16.385131894484413% non-zero values.\n",
      "There are 395/23757 terms that occur more than 30 times in the TF-IDF matrix, with 16.963544303797466% non-zero values.\n",
      "There are 381/23757 terms that occur more than 31 times in the TF-IDF matrix, with 17.359055118110238% non-zero values.\n",
      "There are 370/23757 terms that occur more than 32 times in the TF-IDF matrix, with 17.684864864864867% non-zero values.\n",
      "There are 362/23757 terms that occur more than 33 times in the TF-IDF matrix, with 17.929834254143646% non-zero values.\n",
      "There are 353/23757 terms that occur more than 34 times in the TF-IDF matrix, with 18.213597733711048% non-zero values.\n",
      "There are 339/23757 terms that occur more than 35 times in the TF-IDF matrix, with 18.67669616519174% non-zero values.\n",
      "There are 326/23757 terms that occur more than 36 times in the TF-IDF matrix, with 19.13435582822086% non-zero values.\n",
      "There are 319/23757 terms that occur more than 37 times in the TF-IDF matrix, with 19.391849529780565% non-zero values.\n",
      "There are 309/23757 terms that occur more than 38 times in the TF-IDF matrix, with 19.77346278317152% non-zero values.\n",
      "There are 303/23757 terms that occur more than 39 times in the TF-IDF matrix, with 20.01056105610561% non-zero values.\n"
     ]
    }
   ],
   "source": [
    "# Removing rare features\n",
    "\n",
    "percentage_non_zeros = -1\n",
    "num_features = np.inf\n",
    "threshold = 1\n",
    "full_num_terms = len(denser_df.columns)\n",
    "eligible_features = [term for term in denser_df.columns\n",
    "                     if len(denser_df[denser_df[term] != 0.0]) > threshold]\n",
    "current_df = denser_df[eligible_features]\n",
    "\n",
    "while (percentage_non_zeros < 20) and num_features > 1:\n",
    "    threshold += 1\n",
    "    eligible_features = [term for term in current_df.columns\n",
    "                         if len(current_df[current_df[term] != 0.0]) > threshold]\n",
    "    current_df = current_df[eligible_features]\n",
    "    num_features = len(eligible_features)\n",
    "    percentage_non_zeros = (\n",
    "        len(current_df.to_numpy().nonzero()[0])\n",
    "        / (500 * num_features)\n",
    "        ) * 100\n",
    "    print(f\"There are {num_features}/{full_num_terms} terms that occur more than {threshold} times in the TF-IDF matrix, with {percentage_non_zeros}% non-zero values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d01301",
   "metadata": {},
   "source": [
    "In both cases, trying to reach 1/5 non-zero values seems like a bad idea: On the one hand, we reached 1 feature before we came even close to reaching 20% of non-zero values by reducing the tolerance for low TF-IDF values, and on the other hand, we reached 20% of non-zero values by reducing the tolerance for term rarity, but we end up with only 303 features out of a total of 23,757."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "north_face_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
